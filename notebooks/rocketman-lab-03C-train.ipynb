{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rocketman lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/04/build-first-multi-label-image-classification-model-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\10</td>\n",
       "      <td>c0_04_1_14.PNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\10</td>\n",
       "      <td>c10_18_0_13.PNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\10</td>\n",
       "      <td>c10_18_0_20.PNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\10</td>\n",
       "      <td>c21_05_1_18.PNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\10</td>\n",
       "      <td>c21_05_1_6.PNG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                              path         filename\n",
       "0    10  C:\\Data\\Dev-Data\\music\\labels\\10   c0_04_1_14.PNG\n",
       "1    10  C:\\Data\\Dev-Data\\music\\labels\\10  c10_18_0_13.PNG\n",
       "2    10  C:\\Data\\Dev-Data\\music\\labels\\10  c10_18_0_20.PNG\n",
       "3    10  C:\\Data\\Dev-Data\\music\\labels\\10  c21_05_1_18.PNG\n",
       "4    10  C:\\Data\\Dev-Data\\music\\labels\\10   c21_05_1_6.PNG"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Data\\\\Dev-Data\\\\music\\\\labels\\\\labels.csv\")    \n",
    "df.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>category_10</th>\n",
       "      <th>category_11</th>\n",
       "      <th>category_12</th>\n",
       "      <th>category_13</th>\n",
       "      <th>category_14</th>\n",
       "      <th>category_15</th>\n",
       "      <th>category_16</th>\n",
       "      <th>category_8</th>\n",
       "      <th>category_9</th>\n",
       "      <th>category_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_06_0_14.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_06_0_20.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_06_1_13.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_06_1_3.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_06_1_6.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_08_1_2.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_08_1_7.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_09_1_11.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_10_0_25.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Data\\Dev-Data\\music\\labels\\12</td>\n",
       "      <td>c88_10_1_20.PNG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                              path         filename  category_10  \\\n",
       "250    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_06_0_14.PNG            0   \n",
       "251    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_06_0_20.PNG            0   \n",
       "252    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_06_1_13.PNG            0   \n",
       "253    12  C:\\Data\\Dev-Data\\music\\labels\\12   c88_06_1_3.PNG            0   \n",
       "254    12  C:\\Data\\Dev-Data\\music\\labels\\12   c88_06_1_6.PNG            0   \n",
       "255    12  C:\\Data\\Dev-Data\\music\\labels\\12   c88_08_1_2.PNG            0   \n",
       "256    12  C:\\Data\\Dev-Data\\music\\labels\\12   c88_08_1_7.PNG            0   \n",
       "257    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_09_1_11.PNG            0   \n",
       "258    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_10_0_25.PNG            0   \n",
       "259    12  C:\\Data\\Dev-Data\\music\\labels\\12  c88_10_1_20.PNG            0   \n",
       "\n",
       "     category_11  category_12  category_13  category_14  category_15  \\\n",
       "250            0            1            0            0            0   \n",
       "251            0            1            0            0            0   \n",
       "252            0            1            0            0            0   \n",
       "253            0            1            0            0            0   \n",
       "254            0            1            0            0            0   \n",
       "255            0            1            0            0            0   \n",
       "256            0            1            0            0            0   \n",
       "257            0            1            0            0            0   \n",
       "258            0            1            0            0            0   \n",
       "259            0            1            0            0            0   \n",
       "\n",
       "     category_16  category_8  category_9  category_other  \n",
       "250            0           0           0               0  \n",
       "251            0           0           0               0  \n",
       "252            0           0           0               0  \n",
       "253            0           0           0               0  \n",
       "254            0           0           0               0  \n",
       "255            0           0           0               0  \n",
       "256            0           0           0               0  \n",
       "257            0           0           0               0  \n",
       "258            0           0           0               0  \n",
       "259            0           0           0               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "# http://www.insightsbot.com/blog/zuyVu/python-one-hot-encoding-with-pandas-made-simple \n",
    "\n",
    "df ['label'] = pd.Categorical (df['label'])\n",
    "dfOneHot = pd.get_dummies (df['label'], prefix = 'category')\n",
    "df = pd.concat ([df, dfOneHot], axis=1)\n",
    "\n",
    "df[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'path', 'filename', 'category_10', 'category_11',\n",
       "       'category_12', 'category_13', 'category_14', 'category_15',\n",
       "       'category_16', 'category_8', 'category_9', 'category_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 861/861 [00:01<00:00, 626.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(861, 255, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img(train['path'][i]+'\\\\'+train['filename'][i],target_size=(255,50,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "X = np.array(train_image)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d99faf1908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow (X[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(train.drop(['label', 'path', 'filename'],axis=1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training und validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\georg\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\georg\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# build model architecture\n",
    "\n",
    "# The problems is that the output of layer conv2d_4 became zero or negative. \n",
    "# To solve this problem, you must design the network so that the input data would not be highly downsampled.\n",
    "# Here are some possible solutions:\n",
    "#    Use less layers. Especially remove a max-pooling layer, which downsamples a lot (by one third under this setting).\n",
    "#    Use smaller max-pooling, e.g. pool_size=(2, 2), which results in downsampling by a half.\n",
    "#    Use \"same padding\" for Conv2D layer, which results in no downsampling during the convolution step.\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(400,400,3)))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(255,50,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(25, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 251, 46, 16)       1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 125, 23, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 23, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 23, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 121, 19, 64)       25664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 60, 9, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60, 9, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 5, 64)         102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 28, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               458880    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 597,130\n",
      "Trainable params: 597,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Programs\\Anaconda\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 774 samples, validate on 87 samples\n",
      "Epoch 1/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.8075 - acc: 0.47 - ETA: 10s - loss: 0.7269 - acc: 0.55 - ETA: 8s - loss: 0.6772 - acc: 0.6021 - ETA: 6s - loss: 0.6447 - acc: 0.634 - ETA: 5s - loss: 0.6247 - acc: 0.652 - ETA: 4s - loss: 0.6085 - acc: 0.668 - ETA: 3s - loss: 0.5970 - acc: 0.678 - ETA: 2s - loss: 0.5863 - acc: 0.685 - ETA: 2s - loss: 0.5772 - acc: 0.695 - ETA: 1s - loss: 0.5662 - acc: 0.704 - ETA: 0s - loss: 0.5603 - acc: 0.712 - ETA: 0s - loss: 0.5525 - acc: 0.719 - 9s 11ms/step - loss: 0.5525 - acc: 0.7200 - val_loss: 0.5334 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "774/774 [==============================] - ETA: 6s - loss: 0.4468 - acc: 0.806 - ETA: 6s - loss: 0.4518 - acc: 0.800 - ETA: 5s - loss: 0.4389 - acc: 0.805 - ETA: 5s - loss: 0.4347 - acc: 0.809 - ETA: 4s - loss: 0.4334 - acc: 0.813 - ETA: 4s - loss: 0.4347 - acc: 0.811 - ETA: 3s - loss: 0.4295 - acc: 0.814 - ETA: 2s - loss: 0.4264 - acc: 0.817 - ETA: 2s - loss: 0.4239 - acc: 0.817 - ETA: 1s - loss: 0.4213 - acc: 0.821 - ETA: 0s - loss: 0.4201 - acc: 0.821 - ETA: 0s - loss: 0.4188 - acc: 0.822 - 8s 10ms/step - loss: 0.4180 - acc: 0.8226 - val_loss: 0.4720 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "774/774 [==============================] - ETA: 6s - loss: 0.3834 - acc: 0.829 - ETA: 6s - loss: 0.3986 - acc: 0.839 - ETA: 5s - loss: 0.3913 - acc: 0.845 - ETA: 4s - loss: 0.3836 - acc: 0.852 - ETA: 4s - loss: 0.3855 - acc: 0.851 - ETA: 3s - loss: 0.3869 - acc: 0.850 - ETA: 3s - loss: 0.3910 - acc: 0.847 - ETA: 2s - loss: 0.3900 - acc: 0.847 - ETA: 1s - loss: 0.3880 - acc: 0.847 - ETA: 1s - loss: 0.3834 - acc: 0.848 - ETA: 0s - loss: 0.3802 - acc: 0.850 - ETA: 0s - loss: 0.3785 - acc: 0.852 - 8s 10ms/step - loss: 0.3790 - acc: 0.8516 - val_loss: 0.4944 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "774/774 [==============================] - ETA: 6s - loss: 0.3912 - acc: 0.856 - ETA: 6s - loss: 0.3840 - acc: 0.856 - ETA: 5s - loss: 0.3748 - acc: 0.862 - ETA: 5s - loss: 0.3637 - acc: 0.866 - ETA: 4s - loss: 0.3616 - acc: 0.865 - ETA: 3s - loss: 0.3628 - acc: 0.866 - ETA: 3s - loss: 0.3613 - acc: 0.867 - ETA: 2s - loss: 0.3574 - acc: 0.867 - ETA: 1s - loss: 0.3556 - acc: 0.867 - ETA: 1s - loss: 0.3551 - acc: 0.868 - ETA: 0s - loss: 0.3539 - acc: 0.869 - ETA: 0s - loss: 0.3526 - acc: 0.870 - 8s 10ms/step - loss: 0.3531 - acc: 0.8703 - val_loss: 0.4353 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "774/774 [==============================] - ETA: 7s - loss: 0.3330 - acc: 0.882 - ETA: 6s - loss: 0.3298 - acc: 0.879 - ETA: 5s - loss: 0.3446 - acc: 0.876 - ETA: 5s - loss: 0.3488 - acc: 0.872 - ETA: 4s - loss: 0.3505 - acc: 0.870 - ETA: 3s - loss: 0.3478 - acc: 0.872 - ETA: 3s - loss: 0.3458 - acc: 0.874 - ETA: 2s - loss: 0.3427 - acc: 0.875 - ETA: 1s - loss: 0.3438 - acc: 0.873 - ETA: 1s - loss: 0.3419 - acc: 0.874 - ETA: 0s - loss: 0.3420 - acc: 0.875 - ETA: 0s - loss: 0.3397 - acc: 0.876 - 8s 10ms/step - loss: 0.3394 - acc: 0.8767 - val_loss: 0.4215 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "774/774 [==============================] - ETA: 6s - loss: 0.3720 - acc: 0.868 - ETA: 6s - loss: 0.3336 - acc: 0.880 - ETA: 5s - loss: 0.3394 - acc: 0.877 - ETA: 5s - loss: 0.3354 - acc: 0.883 - ETA: 5s - loss: 0.3318 - acc: 0.883 - ETA: 5s - loss: 0.3276 - acc: 0.883 - ETA: 4s - loss: 0.3250 - acc: 0.886 - ETA: 3s - loss: 0.3210 - acc: 0.886 - ETA: 2s - loss: 0.3205 - acc: 0.886 - ETA: 1s - loss: 0.3200 - acc: 0.886 - ETA: 0s - loss: 0.3188 - acc: 0.887 - ETA: 0s - loss: 0.3204 - acc: 0.887 - 11s 14ms/step - loss: 0.3209 - acc: 0.8867 - val_loss: 0.4265 - val_acc: 0.9069\n",
      "Epoch 7/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.3245 - acc: 0.88 - ETA: 9s - loss: 0.3248 - acc: 0.8820 - ETA: 8s - loss: 0.3216 - acc: 0.881 - ETA: 7s - loss: 0.3162 - acc: 0.887 - ETA: 6s - loss: 0.3179 - acc: 0.886 - ETA: 6s - loss: 0.3201 - acc: 0.886 - ETA: 5s - loss: 0.3198 - acc: 0.886 - ETA: 4s - loss: 0.3158 - acc: 0.887 - ETA: 3s - loss: 0.3146 - acc: 0.888 - ETA: 2s - loss: 0.3128 - acc: 0.888 - ETA: 1s - loss: 0.3116 - acc: 0.889 - ETA: 0s - loss: 0.3088 - acc: 0.891 - 14s 18ms/step - loss: 0.3077 - acc: 0.8917 - val_loss: 0.3643 - val_acc: 0.9195\n",
      "Epoch 8/50\n",
      "774/774 [==============================] - ETA: 13s - loss: 0.2895 - acc: 0.90 - ETA: 12s - loss: 0.3014 - acc: 0.89 - ETA: 10s - loss: 0.3105 - acc: 0.89 - ETA: 10s - loss: 0.3056 - acc: 0.89 - ETA: 11s - loss: 0.3071 - acc: 0.89 - ETA: 10s - loss: 0.3029 - acc: 0.89 - ETA: 9s - loss: 0.3003 - acc: 0.8953 - ETA: 7s - loss: 0.3050 - acc: 0.894 - ETA: 6s - loss: 0.3065 - acc: 0.895 - ETA: 4s - loss: 0.3076 - acc: 0.893 - ETA: 2s - loss: 0.3089 - acc: 0.893 - ETA: 0s - loss: 0.3096 - acc: 0.892 - 33s 42ms/step - loss: 0.3096 - acc: 0.8925 - val_loss: 0.3014 - val_acc: 0.9207\n",
      "Epoch 9/50\n",
      "774/774 [==============================] - ETA: 49s - loss: 0.2967 - acc: 0.89 - ETA: 47s - loss: 0.3171 - acc: 0.89 - ETA: 43s - loss: 0.3134 - acc: 0.89 - ETA: 38s - loss: 0.3179 - acc: 0.89 - ETA: 33s - loss: 0.3124 - acc: 0.89 - ETA: 26s - loss: 0.3108 - acc: 0.89 - ETA: 20s - loss: 0.3041 - acc: 0.89 - ETA: 16s - loss: 0.3014 - acc: 0.90 - ETA: 11s - loss: 0.2996 - acc: 0.90 - ETA: 7s - loss: 0.2970 - acc: 0.9017 - ETA: 3s - loss: 0.2984 - acc: 0.902 - ETA: 0s - loss: 0.2960 - acc: 0.902 - 39s 51ms/step - loss: 0.2957 - acc: 0.9030 - val_loss: 0.2924 - val_acc: 0.9184\n",
      "Epoch 10/50\n",
      "774/774 [==============================] - ETA: 19s - loss: 0.2531 - acc: 0.90 - ETA: 17s - loss: 0.2519 - acc: 0.91 - ETA: 16s - loss: 0.2662 - acc: 0.90 - ETA: 14s - loss: 0.2717 - acc: 0.91 - ETA: 12s - loss: 0.2713 - acc: 0.90 - ETA: 10s - loss: 0.2729 - acc: 0.90 - ETA: 8s - loss: 0.2723 - acc: 0.9083 - ETA: 7s - loss: 0.2717 - acc: 0.907 - ETA: 5s - loss: 0.2718 - acc: 0.908 - ETA: 3s - loss: 0.2717 - acc: 0.908 - ETA: 1s - loss: 0.2729 - acc: 0.908 - ETA: 0s - loss: 0.2771 - acc: 0.907 - 22s 28ms/step - loss: 0.2773 - acc: 0.9068 - val_loss: 0.2738 - val_acc: 0.9195\n",
      "Epoch 11/50\n",
      "774/774 [==============================] - ETA: 19s - loss: 0.3158 - acc: 0.90 - ETA: 17s - loss: 0.2934 - acc: 0.90 - ETA: 15s - loss: 0.2804 - acc: 0.90 - ETA: 13s - loss: 0.2736 - acc: 0.90 - ETA: 12s - loss: 0.2730 - acc: 0.90 - ETA: 10s - loss: 0.2708 - acc: 0.90 - ETA: 8s - loss: 0.2709 - acc: 0.9083 - ETA: 7s - loss: 0.2723 - acc: 0.908 - ETA: 5s - loss: 0.2715 - acc: 0.908 - ETA: 3s - loss: 0.2717 - acc: 0.909 - ETA: 1s - loss: 0.2714 - acc: 0.908 - ETA: 0s - loss: 0.2723 - acc: 0.908 - 20s 26ms/step - loss: 0.2720 - acc: 0.9092 - val_loss: 0.2683 - val_acc: 0.9241\n",
      "Epoch 12/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.2556 - acc: 0.91 - ETA: 13s - loss: 0.2744 - acc: 0.90 - ETA: 11s - loss: 0.2755 - acc: 0.90 - ETA: 10s - loss: 0.2830 - acc: 0.90 - ETA: 9s - loss: 0.2793 - acc: 0.9066 - ETA: 8s - loss: 0.2705 - acc: 0.908 - ETA: 6s - loss: 0.2730 - acc: 0.908 - ETA: 5s - loss: 0.2752 - acc: 0.906 - ETA: 4s - loss: 0.2751 - acc: 0.905 - ETA: 2s - loss: 0.2764 - acc: 0.905 - ETA: 1s - loss: 0.2735 - acc: 0.906 - ETA: 0s - loss: 0.2693 - acc: 0.907 - 16s 21ms/step - loss: 0.2688 - acc: 0.9081 - val_loss: 0.2686 - val_acc: 0.9207\n",
      "Epoch 13/50\n",
      "774/774 [==============================] - ETA: 12s - loss: 0.2989 - acc: 0.90 - ETA: 11s - loss: 0.3134 - acc: 0.90 - ETA: 10s - loss: 0.3011 - acc: 0.90 - ETA: 9s - loss: 0.2947 - acc: 0.9023 - ETA: 8s - loss: 0.2890 - acc: 0.904 - ETA: 6s - loss: 0.2814 - acc: 0.907 - ETA: 5s - loss: 0.2748 - acc: 0.908 - ETA: 4s - loss: 0.2741 - acc: 0.908 - ETA: 3s - loss: 0.2743 - acc: 0.908 - ETA: 2s - loss: 0.2709 - acc: 0.909 - ETA: 1s - loss: 0.2697 - acc: 0.910 - ETA: 0s - loss: 0.2699 - acc: 0.910 - 14s 18ms/step - loss: 0.2692 - acc: 0.9101 - val_loss: 0.2374 - val_acc: 0.9310\n",
      "Epoch 14/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.2571 - acc: 0.92 - ETA: 10s - loss: 0.2629 - acc: 0.91 - ETA: 9s - loss: 0.2533 - acc: 0.9229 - ETA: 8s - loss: 0.2536 - acc: 0.921 - ETA: 7s - loss: 0.2587 - acc: 0.919 - ETA: 6s - loss: 0.2641 - acc: 0.915 - ETA: 5s - loss: 0.2613 - acc: 0.914 - ETA: 4s - loss: 0.2592 - acc: 0.913 - ETA: 3s - loss: 0.2570 - acc: 0.914 - ETA: 2s - loss: 0.2557 - acc: 0.914 - ETA: 1s - loss: 0.2584 - acc: 0.913 - ETA: 0s - loss: 0.2575 - acc: 0.914 - 13s 17ms/step - loss: 0.2578 - acc: 0.9142 - val_loss: 0.2187 - val_acc: 0.9310\n",
      "Epoch 15/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2162 - acc: 0.92 - ETA: 9s - loss: 0.2453 - acc: 0.9203 - ETA: 8s - loss: 0.2466 - acc: 0.921 - ETA: 7s - loss: 0.2466 - acc: 0.923 - ETA: 6s - loss: 0.2468 - acc: 0.920 - ETA: 5s - loss: 0.2445 - acc: 0.920 - ETA: 4s - loss: 0.2433 - acc: 0.921 - ETA: 3s - loss: 0.2393 - acc: 0.921 - ETA: 2s - loss: 0.2418 - acc: 0.920 - ETA: 2s - loss: 0.2446 - acc: 0.920 - ETA: 1s - loss: 0.2439 - acc: 0.920 - ETA: 0s - loss: 0.2448 - acc: 0.919 - 12s 16ms/step - loss: 0.2447 - acc: 0.9191 - val_loss: 0.2495 - val_acc: 0.9161\n",
      "Epoch 16/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.2590 - acc: 0.91 - ETA: 9s - loss: 0.2484 - acc: 0.9102 - ETA: 8s - loss: 0.2421 - acc: 0.913 - ETA: 7s - loss: 0.2447 - acc: 0.914 - ETA: 6s - loss: 0.2383 - acc: 0.916 - ETA: 5s - loss: 0.2371 - acc: 0.918 - ETA: 4s - loss: 0.2448 - acc: 0.916 - ETA: 3s - loss: 0.2444 - acc: 0.916 - ETA: 2s - loss: 0.2438 - acc: 0.916 - ETA: 1s - loss: 0.2449 - acc: 0.916 - ETA: 1s - loss: 0.2444 - acc: 0.917 - ETA: 0s - loss: 0.2409 - acc: 0.918 - 12s 15ms/step - loss: 0.2418 - acc: 0.9183 - val_loss: 0.2088 - val_acc: 0.9322\n",
      "Epoch 17/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2473 - acc: 0.91 - ETA: 9s - loss: 0.2324 - acc: 0.9211 - ETA: 8s - loss: 0.2226 - acc: 0.924 - ETA: 7s - loss: 0.2311 - acc: 0.918 - ETA: 6s - loss: 0.2276 - acc: 0.919 - ETA: 5s - loss: 0.2309 - acc: 0.920 - ETA: 4s - loss: 0.2297 - acc: 0.920 - ETA: 3s - loss: 0.2280 - acc: 0.919 - ETA: 2s - loss: 0.2303 - acc: 0.919 - ETA: 2s - loss: 0.2315 - acc: 0.919 - ETA: 1s - loss: 0.2315 - acc: 0.918 - ETA: 0s - loss: 0.2300 - acc: 0.919 - 12s 15ms/step - loss: 0.2302 - acc: 0.9194 - val_loss: 0.2049 - val_acc: 0.9333\n",
      "Epoch 18/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2581 - acc: 0.91 - ETA: 9s - loss: 0.2568 - acc: 0.9156 - ETA: 8s - loss: 0.2391 - acc: 0.922 - ETA: 7s - loss: 0.2318 - acc: 0.923 - ETA: 6s - loss: 0.2259 - acc: 0.925 - ETA: 5s - loss: 0.2249 - acc: 0.923 - ETA: 4s - loss: 0.2219 - acc: 0.923 - ETA: 3s - loss: 0.2228 - acc: 0.924 - ETA: 2s - loss: 0.2258 - acc: 0.923 - ETA: 1s - loss: 0.2253 - acc: 0.924 - ETA: 1s - loss: 0.2280 - acc: 0.924 - ETA: 0s - loss: 0.2272 - acc: 0.924 - 12s 15ms/step - loss: 0.2267 - acc: 0.9249 - val_loss: 0.1791 - val_acc: 0.9391\n",
      "Epoch 19/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2036 - acc: 0.92 - ETA: 9s - loss: 0.2150 - acc: 0.9250 - ETA: 8s - loss: 0.2133 - acc: 0.929 - ETA: 7s - loss: 0.2219 - acc: 0.922 - ETA: 6s - loss: 0.2174 - acc: 0.926 - ETA: 5s - loss: 0.2185 - acc: 0.925 - ETA: 4s - loss: 0.2184 - acc: 0.925 - ETA: 3s - loss: 0.2191 - acc: 0.925 - ETA: 2s - loss: 0.2180 - acc: 0.926 - ETA: 1s - loss: 0.2188 - acc: 0.926 - ETA: 1s - loss: 0.2185 - acc: 0.925 - ETA: 0s - loss: 0.2184 - acc: 0.925 - 12s 15ms/step - loss: 0.2177 - acc: 0.9251 - val_loss: 0.1785 - val_acc: 0.9379\n",
      "Epoch 20/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2103 - acc: 0.91 - ETA: 9s - loss: 0.2010 - acc: 0.9273 - ETA: 8s - loss: 0.2057 - acc: 0.927 - ETA: 7s - loss: 0.1964 - acc: 0.929 - ETA: 7s - loss: 0.1898 - acc: 0.931 - ETA: 5s - loss: 0.1937 - acc: 0.929 - ETA: 4s - loss: 0.1965 - acc: 0.929 - ETA: 3s - loss: 0.1950 - acc: 0.929 - ETA: 2s - loss: 0.1964 - acc: 0.928 - ETA: 2s - loss: 0.1959 - acc: 0.928 - ETA: 1s - loss: 0.1961 - acc: 0.929 - ETA: 0s - loss: 0.1965 - acc: 0.928 - 12s 15ms/step - loss: 0.1965 - acc: 0.9283 - val_loss: 0.1713 - val_acc: 0.9425\n",
      "Epoch 21/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.1732 - acc: 0.94 - ETA: 9s - loss: 0.1705 - acc: 0.9438 - ETA: 8s - loss: 0.1754 - acc: 0.942 - ETA: 7s - loss: 0.1816 - acc: 0.938 - ETA: 6s - loss: 0.1870 - acc: 0.937 - ETA: 6s - loss: 0.1863 - acc: 0.936 - ETA: 5s - loss: 0.1899 - acc: 0.934 - ETA: 4s - loss: 0.1893 - acc: 0.932 - ETA: 3s - loss: 0.1927 - acc: 0.932 - ETA: 2s - loss: 0.1934 - acc: 0.931 - ETA: 1s - loss: 0.1938 - acc: 0.930 - ETA: 0s - loss: 0.1942 - acc: 0.929 - 12s 16ms/step - loss: 0.1939 - acc: 0.9298 - val_loss: 0.1587 - val_acc: 0.9483\n",
      "Epoch 22/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.2133 - acc: 0.93 - ETA: 9s - loss: 0.1866 - acc: 0.9398 - ETA: 9s - loss: 0.1856 - acc: 0.937 - ETA: 8s - loss: 0.1983 - acc: 0.931 - ETA: 7s - loss: 0.1932 - acc: 0.931 - ETA: 6s - loss: 0.1957 - acc: 0.930 - ETA: 5s - loss: 0.1989 - acc: 0.929 - ETA: 4s - loss: 0.1982 - acc: 0.929 - ETA: 3s - loss: 0.1985 - acc: 0.928 - ETA: 2s - loss: 0.1968 - acc: 0.929 - ETA: 1s - loss: 0.1938 - acc: 0.930 - ETA: 0s - loss: 0.1935 - acc: 0.930 - 12s 16ms/step - loss: 0.1932 - acc: 0.9302 - val_loss: 0.1475 - val_acc: 0.9425\n",
      "Epoch 23/50\n",
      "774/774 [==============================] - ETA: 10s - loss: 0.1761 - acc: 0.92 - ETA: 9s - loss: 0.1852 - acc: 0.9359 - ETA: 8s - loss: 0.1953 - acc: 0.931 - ETA: 7s - loss: 0.1931 - acc: 0.932 - ETA: 6s - loss: 0.1863 - acc: 0.934 - ETA: 5s - loss: 0.1790 - acc: 0.937 - ETA: 4s - loss: 0.1820 - acc: 0.935 - ETA: 3s - loss: 0.1816 - acc: 0.936 - ETA: 2s - loss: 0.1813 - acc: 0.935 - ETA: 2s - loss: 0.1815 - acc: 0.934 - ETA: 1s - loss: 0.1829 - acc: 0.933 - ETA: 0s - loss: 0.1827 - acc: 0.933 - 12s 16ms/step - loss: 0.1822 - acc: 0.9336 - val_loss: 0.1515 - val_acc: 0.9471\n",
      "Epoch 24/50\n",
      "774/774 [==============================] - ETA: 12s - loss: 0.1281 - acc: 0.95 - ETA: 10s - loss: 0.1512 - acc: 0.94 - ETA: 9s - loss: 0.1577 - acc: 0.9375 - ETA: 8s - loss: 0.1604 - acc: 0.939 - ETA: 7s - loss: 0.1631 - acc: 0.937 - ETA: 6s - loss: 0.1660 - acc: 0.937 - ETA: 5s - loss: 0.1738 - acc: 0.935 - ETA: 4s - loss: 0.1727 - acc: 0.935 - ETA: 3s - loss: 0.1745 - acc: 0.934 - ETA: 2s - loss: 0.1740 - acc: 0.933 - ETA: 1s - loss: 0.1775 - acc: 0.932 - ETA: 0s - loss: 0.1768 - acc: 0.932 - 13s 16ms/step - loss: 0.1770 - acc: 0.9322 - val_loss: 0.1441 - val_acc: 0.9437\n",
      "Epoch 25/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.1488 - acc: 0.93 - ETA: 10s - loss: 0.1648 - acc: 0.93 - ETA: 9s - loss: 0.1803 - acc: 0.9302 - ETA: 8s - loss: 0.1839 - acc: 0.929 - ETA: 7s - loss: 0.1831 - acc: 0.930 - ETA: 6s - loss: 0.1819 - acc: 0.931 - ETA: 5s - loss: 0.1784 - acc: 0.933 - ETA: 4s - loss: 0.1791 - acc: 0.932 - ETA: 3s - loss: 0.1784 - acc: 0.932 - ETA: 2s - loss: 0.1775 - acc: 0.933 - ETA: 1s - loss: 0.1773 - acc: 0.933 - ETA: 0s - loss: 0.1749 - acc: 0.934 - 13s 17ms/step - loss: 0.1745 - acc: 0.9344 - val_loss: 0.1428 - val_acc: 0.9494\n",
      "Epoch 26/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.1776 - acc: 0.91 - ETA: 10s - loss: 0.2021 - acc: 0.92 - ETA: 9s - loss: 0.1939 - acc: 0.9260 - ETA: 8s - loss: 0.1985 - acc: 0.926 - ETA: 7s - loss: 0.1858 - acc: 0.930 - ETA: 6s - loss: 0.1843 - acc: 0.929 - ETA: 5s - loss: 0.1840 - acc: 0.929 - ETA: 4s - loss: 0.1838 - acc: 0.927 - ETA: 3s - loss: 0.1835 - acc: 0.928 - ETA: 2s - loss: 0.1793 - acc: 0.929 - ETA: 1s - loss: 0.1749 - acc: 0.931 - ETA: 0s - loss: 0.1720 - acc: 0.932 - 13s 17ms/step - loss: 0.1713 - acc: 0.9329 - val_loss: 0.1390 - val_acc: 0.9494\n",
      "Epoch 27/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.1707 - acc: 0.93 - ETA: 10s - loss: 0.1664 - acc: 0.93 - ETA: 9s - loss: 0.1619 - acc: 0.9396 - ETA: 8s - loss: 0.1631 - acc: 0.937 - ETA: 7s - loss: 0.1612 - acc: 0.938 - ETA: 6s - loss: 0.1583 - acc: 0.939 - ETA: 5s - loss: 0.1576 - acc: 0.938 - ETA: 4s - loss: 0.1558 - acc: 0.939 - ETA: 3s - loss: 0.1546 - acc: 0.940 - ETA: 2s - loss: 0.1593 - acc: 0.938 - ETA: 1s - loss: 0.1619 - acc: 0.937 - ETA: 0s - loss: 0.1609 - acc: 0.937 - 14s 18ms/step - loss: 0.1604 - acc: 0.9373 - val_loss: 0.1268 - val_acc: 0.9506\n",
      "Epoch 28/50\n",
      "774/774 [==============================] - ETA: 11s - loss: 0.1369 - acc: 0.94 - ETA: 10s - loss: 0.1519 - acc: 0.93 - ETA: 9s - loss: 0.1541 - acc: 0.9375 - ETA: 8s - loss: 0.1609 - acc: 0.935 - ETA: 7s - loss: 0.1621 - acc: 0.936 - ETA: 6s - loss: 0.1702 - acc: 0.933 - ETA: 5s - loss: 0.1701 - acc: 0.932 - ETA: 4s - loss: 0.1657 - acc: 0.934 - ETA: 3s - loss: 0.1643 - acc: 0.934 - ETA: 2s - loss: 0.1632 - acc: 0.935 - ETA: 1s - loss: 0.1636 - acc: 0.935 - ETA: 0s - loss: 0.1626 - acc: 0.935 - 14s 18ms/step - loss: 0.1624 - acc: 0.9355 - val_loss: 0.1313 - val_acc: 0.9517\n",
      "Epoch 29/50\n",
      "774/774 [==============================] - ETA: 13s - loss: 0.1429 - acc: 0.94 - ETA: 11s - loss: 0.1364 - acc: 0.94 - ETA: 10s - loss: 0.1386 - acc: 0.94 - ETA: 9s - loss: 0.1454 - acc: 0.9387 - ETA: 8s - loss: 0.1410 - acc: 0.939 - ETA: 6s - loss: 0.1464 - acc: 0.938 - ETA: 5s - loss: 0.1513 - acc: 0.937 - ETA: 4s - loss: 0.1511 - acc: 0.938 - ETA: 3s - loss: 0.1468 - acc: 0.940 - ETA: 2s - loss: 0.1524 - acc: 0.937 - ETA: 1s - loss: 0.1507 - acc: 0.937 - ETA: 0s - loss: 0.1500 - acc: 0.937 - 14s 19ms/step - loss: 0.1505 - acc: 0.9377 - val_loss: 0.1299 - val_acc: 0.9540\n",
      "Epoch 30/50\n",
      "774/774 [==============================] - ETA: 13s - loss: 0.1711 - acc: 0.93 - ETA: 12s - loss: 0.1624 - acc: 0.93 - ETA: 11s - loss: 0.1555 - acc: 0.93 - ETA: 9s - loss: 0.1608 - acc: 0.9309 - ETA: 8s - loss: 0.1662 - acc: 0.931 - ETA: 7s - loss: 0.1642 - acc: 0.933 - ETA: 6s - loss: 0.1571 - acc: 0.937 - ETA: 4s - loss: 0.1588 - acc: 0.937 - ETA: 3s - loss: 0.1603 - acc: 0.937 - ETA: 2s - loss: 0.1558 - acc: 0.939 - ETA: 1s - loss: 0.1528 - acc: 0.940 - ETA: 0s - loss: 0.1537 - acc: 0.940 - 15s 19ms/step - loss: 0.1536 - acc: 0.9406 - val_loss: 0.1285 - val_acc: 0.9471\n",
      "Epoch 31/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1215 - acc: 0.95 - ETA: 12s - loss: 0.1305 - acc: 0.94 - ETA: 11s - loss: 0.1313 - acc: 0.94 - ETA: 10s - loss: 0.1348 - acc: 0.94 - ETA: 8s - loss: 0.1325 - acc: 0.9447 - ETA: 7s - loss: 0.1396 - acc: 0.942 - ETA: 6s - loss: 0.1405 - acc: 0.942 - ETA: 5s - loss: 0.1423 - acc: 0.941 - ETA: 3s - loss: 0.1430 - acc: 0.940 - ETA: 2s - loss: 0.1421 - acc: 0.940 - ETA: 1s - loss: 0.1413 - acc: 0.941 - ETA: 0s - loss: 0.1430 - acc: 0.940 - 15s 20ms/step - loss: 0.1423 - acc: 0.9410 - val_loss: 0.1152 - val_acc: 0.9529\n",
      "Epoch 32/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1484 - acc: 0.93 - ETA: 13s - loss: 0.1581 - acc: 0.93 - ETA: 11s - loss: 0.1519 - acc: 0.93 - ETA: 10s - loss: 0.1526 - acc: 0.93 - ETA: 9s - loss: 0.1495 - acc: 0.9406 - ETA: 7s - loss: 0.1495 - acc: 0.940 - ETA: 6s - loss: 0.1450 - acc: 0.943 - ETA: 5s - loss: 0.1439 - acc: 0.943 - ETA: 3s - loss: 0.1418 - acc: 0.943 - ETA: 2s - loss: 0.1429 - acc: 0.943 - ETA: 1s - loss: 0.1447 - acc: 0.941 - ETA: 0s - loss: 0.1417 - acc: 0.943 - 16s 20ms/step - loss: 0.1419 - acc: 0.9432 - val_loss: 0.1097 - val_acc: 0.9575\n",
      "Epoch 33/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1665 - acc: 0.93 - ETA: 13s - loss: 0.1540 - acc: 0.94 - ETA: 12s - loss: 0.1398 - acc: 0.94 - ETA: 10s - loss: 0.1381 - acc: 0.94 - ETA: 9s - loss: 0.1389 - acc: 0.9488 - ETA: 7s - loss: 0.1361 - acc: 0.948 - ETA: 6s - loss: 0.1406 - acc: 0.944 - ETA: 5s - loss: 0.1407 - acc: 0.944 - ETA: 3s - loss: 0.1378 - acc: 0.946 - ETA: 2s - loss: 0.1363 - acc: 0.946 - ETA: 1s - loss: 0.1367 - acc: 0.945 - ETA: 0s - loss: 0.1364 - acc: 0.945 - 16s 21ms/step - loss: 0.1359 - acc: 0.9452 - val_loss: 0.1184 - val_acc: 0.9494\n",
      "Epoch 34/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1400 - acc: 0.93 - ETA: 13s - loss: 0.1367 - acc: 0.94 - ETA: 12s - loss: 0.1384 - acc: 0.93 - ETA: 10s - loss: 0.1339 - acc: 0.94 - ETA: 9s - loss: 0.1312 - acc: 0.9438 - ETA: 8s - loss: 0.1286 - acc: 0.943 - ETA: 6s - loss: 0.1310 - acc: 0.943 - ETA: 5s - loss: 0.1324 - acc: 0.944 - ETA: 3s - loss: 0.1356 - acc: 0.942 - ETA: 2s - loss: 0.1349 - acc: 0.942 - ETA: 1s - loss: 0.1339 - acc: 0.943 - ETA: 0s - loss: 0.1361 - acc: 0.941 - 17s 22ms/step - loss: 0.1359 - acc: 0.9420 - val_loss: 0.1237 - val_acc: 0.9506\n",
      "Epoch 35/50\n",
      "774/774 [==============================] - ETA: 16s - loss: 0.1208 - acc: 0.95 - ETA: 14s - loss: 0.1259 - acc: 0.95 - ETA: 12s - loss: 0.1200 - acc: 0.95 - ETA: 11s - loss: 0.1234 - acc: 0.95 - ETA: 9s - loss: 0.1267 - acc: 0.9488 - ETA: 8s - loss: 0.1286 - acc: 0.946 - ETA: 7s - loss: 0.1352 - acc: 0.944 - ETA: 5s - loss: 0.1357 - acc: 0.943 - ETA: 4s - loss: 0.1331 - acc: 0.944 - ETA: 2s - loss: 0.1320 - acc: 0.945 - ETA: 1s - loss: 0.1318 - acc: 0.946 - ETA: 0s - loss: 0.1317 - acc: 0.946 - 18s 23ms/step - loss: 0.1325 - acc: 0.9461 - val_loss: 0.1176 - val_acc: 0.9540\n",
      "Epoch 36/50\n",
      "774/774 [==============================] - ETA: 16s - loss: 0.1290 - acc: 0.94 - ETA: 15s - loss: 0.1206 - acc: 0.94 - ETA: 14s - loss: 0.1236 - acc: 0.94 - ETA: 13s - loss: 0.1219 - acc: 0.94 - ETA: 12s - loss: 0.1233 - acc: 0.94 - ETA: 10s - loss: 0.1255 - acc: 0.94 - ETA: 8s - loss: 0.1271 - acc: 0.9455 - ETA: 6s - loss: 0.1271 - acc: 0.945 - ETA: 4s - loss: 0.1263 - acc: 0.945 - ETA: 3s - loss: 0.1256 - acc: 0.946 - ETA: 1s - loss: 0.1238 - acc: 0.946 - ETA: 0s - loss: 0.1254 - acc: 0.945 - 21s 28ms/step - loss: 0.1254 - acc: 0.9453 - val_loss: 0.1115 - val_acc: 0.9552\n",
      "Epoch 37/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1279 - acc: 0.94 - ETA: 13s - loss: 0.1231 - acc: 0.94 - ETA: 12s - loss: 0.1224 - acc: 0.94 - ETA: 11s - loss: 0.1235 - acc: 0.94 - ETA: 9s - loss: 0.1254 - acc: 0.9472 - ETA: 8s - loss: 0.1232 - acc: 0.949 - ETA: 7s - loss: 0.1285 - acc: 0.948 - ETA: 5s - loss: 0.1272 - acc: 0.948 - ETA: 4s - loss: 0.1260 - acc: 0.948 - ETA: 3s - loss: 0.1267 - acc: 0.948 - ETA: 1s - loss: 0.1247 - acc: 0.948 - ETA: 0s - loss: 0.1270 - acc: 0.948 - 18s 23ms/step - loss: 0.1268 - acc: 0.9483 - val_loss: 0.1287 - val_acc: 0.9483\n",
      "Epoch 38/50\n",
      "774/774 [==============================] - ETA: 20s - loss: 0.1450 - acc: 0.95 - ETA: 16s - loss: 0.1517 - acc: 0.93 - ETA: 14s - loss: 0.1340 - acc: 0.94 - ETA: 12s - loss: 0.1296 - acc: 0.94 - ETA: 10s - loss: 0.1306 - acc: 0.94 - ETA: 9s - loss: 0.1291 - acc: 0.9482 - ETA: 7s - loss: 0.1287 - acc: 0.948 - ETA: 6s - loss: 0.1265 - acc: 0.949 - ETA: 4s - loss: 0.1235 - acc: 0.950 - ETA: 3s - loss: 0.1227 - acc: 0.950 - ETA: 1s - loss: 0.1203 - acc: 0.951 - ETA: 0s - loss: 0.1183 - acc: 0.951 - 19s 24ms/step - loss: 0.1184 - acc: 0.9516 - val_loss: 0.1071 - val_acc: 0.9517\n",
      "Epoch 39/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1070 - acc: 0.95 - ETA: 13s - loss: 0.1008 - acc: 0.95 - ETA: 13s - loss: 0.1015 - acc: 0.95 - ETA: 11s - loss: 0.1062 - acc: 0.95 - ETA: 10s - loss: 0.1122 - acc: 0.95 - ETA: 9s - loss: 0.1083 - acc: 0.9534 - ETA: 7s - loss: 0.1091 - acc: 0.952 - ETA: 6s - loss: 0.1092 - acc: 0.952 - ETA: 4s - loss: 0.1079 - acc: 0.952 - ETA: 3s - loss: 0.1071 - acc: 0.952 - ETA: 1s - loss: 0.1082 - acc: 0.952 - ETA: 0s - loss: 0.1103 - acc: 0.952 - 20s 25ms/step - loss: 0.1103 - acc: 0.9523 - val_loss: 0.1133 - val_acc: 0.9506\n",
      "Epoch 40/50\n",
      "774/774 [==============================] - ETA: 17s - loss: 0.1375 - acc: 0.94 - ETA: 15s - loss: 0.1290 - acc: 0.94 - ETA: 14s - loss: 0.1200 - acc: 0.95 - ETA: 12s - loss: 0.1129 - acc: 0.95 - ETA: 10s - loss: 0.1116 - acc: 0.95 - ETA: 9s - loss: 0.1176 - acc: 0.9513 - ETA: 7s - loss: 0.1125 - acc: 0.952 - ETA: 6s - loss: 0.1114 - acc: 0.952 - ETA: 4s - loss: 0.1085 - acc: 0.952 - ETA: 3s - loss: 0.1102 - acc: 0.952 - ETA: 1s - loss: 0.1098 - acc: 0.952 - ETA: 0s - loss: 0.1112 - acc: 0.951 - 19s 25ms/step - loss: 0.1113 - acc: 0.9513 - val_loss: 0.1079 - val_acc: 0.9552\n",
      "Epoch 41/50\n",
      "774/774 [==============================] - ETA: 16s - loss: 0.1132 - acc: 0.94 - ETA: 17s - loss: 0.1068 - acc: 0.95 - ETA: 15s - loss: 0.1082 - acc: 0.95 - ETA: 13s - loss: 0.1109 - acc: 0.95 - ETA: 11s - loss: 0.1172 - acc: 0.95 - ETA: 9s - loss: 0.1129 - acc: 0.9544 - ETA: 8s - loss: 0.1095 - acc: 0.956 - ETA: 6s - loss: 0.1099 - acc: 0.954 - ETA: 4s - loss: 0.1093 - acc: 0.955 - ETA: 3s - loss: 0.1073 - acc: 0.955 - ETA: 1s - loss: 0.1090 - acc: 0.955 - ETA: 0s - loss: 0.1104 - acc: 0.954 - 20s 26ms/step - loss: 0.1097 - acc: 0.9549 - val_loss: 0.1085 - val_acc: 0.9529\n",
      "Epoch 42/50\n",
      "774/774 [==============================] - ETA: 16s - loss: 0.1135 - acc: 0.95 - ETA: 14s - loss: 0.0993 - acc: 0.95 - ETA: 13s - loss: 0.1009 - acc: 0.95 - ETA: 11s - loss: 0.1041 - acc: 0.95 - ETA: 10s - loss: 0.0981 - acc: 0.95 - ETA: 8s - loss: 0.1001 - acc: 0.9547 - ETA: 7s - loss: 0.0994 - acc: 0.954 - ETA: 6s - loss: 0.1083 - acc: 0.953 - ETA: 4s - loss: 0.1078 - acc: 0.953 - ETA: 3s - loss: 0.1083 - acc: 0.952 - ETA: 1s - loss: 0.1091 - acc: 0.951 - ETA: 0s - loss: 0.1112 - acc: 0.951 - 18s 24ms/step - loss: 0.1109 - acc: 0.9518 - val_loss: 0.1089 - val_acc: 0.9575\n",
      "Epoch 43/50\n",
      "774/774 [==============================] - ETA: 15s - loss: 0.1077 - acc: 0.94 - ETA: 15s - loss: 0.0965 - acc: 0.95 - ETA: 14s - loss: 0.0950 - acc: 0.96 - ETA: 12s - loss: 0.0991 - acc: 0.96 - ETA: 10s - loss: 0.1044 - acc: 0.95 - ETA: 9s - loss: 0.1025 - acc: 0.9586 - ETA: 7s - loss: 0.1040 - acc: 0.958 - ETA: 6s - loss: 0.1044 - acc: 0.957 - ETA: 4s - loss: 0.1028 - acc: 0.957 - ETA: 3s - loss: 0.1053 - acc: 0.957 - ETA: 1s - loss: 0.1058 - acc: 0.956 - ETA: 0s - loss: 0.1067 - acc: 0.955 - 18s 23ms/step - loss: 0.1063 - acc: 0.9559 - val_loss: 0.1036 - val_acc: 0.9563\n",
      "Epoch 44/50\n",
      "774/774 [==============================] - ETA: 14s - loss: 0.1037 - acc: 0.95 - ETA: 13s - loss: 0.1001 - acc: 0.95 - ETA: 11s - loss: 0.1065 - acc: 0.95 - ETA: 10s - loss: 0.1047 - acc: 0.95 - ETA: 10s - loss: 0.0984 - acc: 0.95 - ETA: 8s - loss: 0.1033 - acc: 0.9549 - ETA: 7s - loss: 0.1003 - acc: 0.956 - ETA: 5s - loss: 0.1010 - acc: 0.955 - ETA: 4s - loss: 0.0987 - acc: 0.956 - ETA: 2s - loss: 0.0983 - acc: 0.957 - ETA: 1s - loss: 0.0971 - acc: 0.957 - ETA: 0s - loss: 0.0970 - acc: 0.957 - 18s 23ms/step - loss: 0.0979 - acc: 0.9567 - val_loss: 0.0973 - val_acc: 0.9586\n",
      "Epoch 45/50\n",
      "774/774 [==============================] - ETA: 15s - loss: 0.0711 - acc: 0.96 - ETA: 15s - loss: 0.0950 - acc: 0.95 - ETA: 14s - loss: 0.1052 - acc: 0.95 - ETA: 13s - loss: 0.0991 - acc: 0.95 - ETA: 11s - loss: 0.0967 - acc: 0.95 - ETA: 9s - loss: 0.0968 - acc: 0.9586 - ETA: 7s - loss: 0.0928 - acc: 0.959 - ETA: 6s - loss: 0.0948 - acc: 0.959 - ETA: 4s - loss: 0.0931 - acc: 0.959 - ETA: 3s - loss: 0.0926 - acc: 0.960 - ETA: 1s - loss: 0.0948 - acc: 0.959 - ETA: 0s - loss: 0.0982 - acc: 0.957 - 18s 23ms/step - loss: 0.0980 - acc: 0.9574 - val_loss: 0.1022 - val_acc: 0.9552\n",
      "Epoch 46/50\n",
      "774/774 [==============================] - ETA: 15s - loss: 0.0784 - acc: 0.95 - ETA: 14s - loss: 0.0930 - acc: 0.95 - ETA: 12s - loss: 0.0821 - acc: 0.95 - ETA: 11s - loss: 0.0872 - acc: 0.95 - ETA: 9s - loss: 0.0920 - acc: 0.9584 - ETA: 8s - loss: 0.0904 - acc: 0.959 - ETA: 6s - loss: 0.0954 - acc: 0.958 - ETA: 5s - loss: 0.0969 - acc: 0.959 - ETA: 4s - loss: 0.0988 - acc: 0.959 - ETA: 2s - loss: 0.0984 - acc: 0.959 - ETA: 1s - loss: 0.0963 - acc: 0.959 - ETA: 0s - loss: 0.0988 - acc: 0.959 - 17s 22ms/step - loss: 0.0981 - acc: 0.9598 - val_loss: 0.1027 - val_acc: 0.9575\n",
      "Epoch 47/50\n",
      "774/774 [==============================] - ETA: 15s - loss: 0.1053 - acc: 0.96 - ETA: 14s - loss: 0.0976 - acc: 0.96 - ETA: 13s - loss: 0.0990 - acc: 0.96 - ETA: 11s - loss: 0.0951 - acc: 0.96 - ETA: 12s - loss: 0.0917 - acc: 0.96 - ETA: 11s - loss: 0.0952 - acc: 0.96 - ETA: 10s - loss: 0.1008 - acc: 0.95 - ETA: 8s - loss: 0.0968 - acc: 0.9604 - ETA: 6s - loss: 0.0930 - acc: 0.961 - ETA: 4s - loss: 0.0903 - acc: 0.962 - ETA: 2s - loss: 0.0902 - acc: 0.962 - ETA: 0s - loss: 0.0912 - acc: 0.962 - 24s 31ms/step - loss: 0.0909 - acc: 0.9624 - val_loss: 0.0930 - val_acc: 0.9655\n",
      "Epoch 48/50\n",
      "774/774 [==============================] - ETA: 15s - loss: 0.0782 - acc: 0.95 - ETA: 14s - loss: 0.0815 - acc: 0.95 - ETA: 13s - loss: 0.0855 - acc: 0.95 - ETA: 11s - loss: 0.0882 - acc: 0.95 - ETA: 10s - loss: 0.0907 - acc: 0.96 - ETA: 9s - loss: 0.0881 - acc: 0.9609 - ETA: 7s - loss: 0.0863 - acc: 0.962 - ETA: 6s - loss: 0.0859 - acc: 0.961 - ETA: 4s - loss: 0.0849 - acc: 0.962 - ETA: 3s - loss: 0.0834 - acc: 0.963 - ETA: 1s - loss: 0.0834 - acc: 0.963 - ETA: 0s - loss: 0.0829 - acc: 0.963 - 19s 24ms/step - loss: 0.0833 - acc: 0.9633 - val_loss: 0.0881 - val_acc: 0.9575\n",
      "Epoch 49/50\n",
      "774/774 [==============================] - ETA: 22s - loss: 0.0883 - acc: 0.96 - ETA: 21s - loss: 0.0898 - acc: 0.96 - ETA: 18s - loss: 0.1031 - acc: 0.95 - ETA: 16s - loss: 0.0945 - acc: 0.96 - ETA: 14s - loss: 0.0967 - acc: 0.96 - ETA: 11s - loss: 0.0922 - acc: 0.96 - ETA: 9s - loss: 0.0964 - acc: 0.9600 - ETA: 7s - loss: 0.0981 - acc: 0.960 - ETA: 5s - loss: 0.0971 - acc: 0.960 - ETA: 3s - loss: 0.0960 - acc: 0.961 - ETA: 1s - loss: 0.0957 - acc: 0.960 - ETA: 0s - loss: 0.0955 - acc: 0.960 - 20s 26ms/step - loss: 0.0958 - acc: 0.9606 - val_loss: 0.0961 - val_acc: 0.9609\n",
      "Epoch 50/50\n",
      "774/774 [==============================] - ETA: 18s - loss: 0.0780 - acc: 0.96 - ETA: 16s - loss: 0.0873 - acc: 0.96 - ETA: 15s - loss: 0.0787 - acc: 0.96 - ETA: 13s - loss: 0.0807 - acc: 0.96 - ETA: 11s - loss: 0.0842 - acc: 0.96 - ETA: 9s - loss: 0.0826 - acc: 0.9667 - ETA: 7s - loss: 0.0800 - acc: 0.968 - ETA: 6s - loss: 0.0832 - acc: 0.967 - ETA: 5s - loss: 0.0851 - acc: 0.966 - ETA: 3s - loss: 0.0860 - acc: 0.965 - ETA: 1s - loss: 0.0850 - acc: 0.966 - ETA: 0s - loss: 0.0852 - acc: 0.965 - 21s 28ms/step - loss: 0.0850 - acc: 0.9656 - val_loss: 0.1013 - val_acc: 0.9621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d9a7a66848>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"C:\\\\Data\\\\Dev-Data\\\\music\\\\model\\\\model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"C:\\\\Data\\\\Dev-Data\\\\music\\\\model\\\\model.h5\")\n",
    "\n",
    "# serialize model columns / labels\n",
    "train.to_csv(\"C:\\\\Data\\\\Dev-Data\\\\music\\\\model\\\\model-columns.csv\", index=None, header=True)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
